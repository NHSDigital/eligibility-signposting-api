import json
import logging
import os
from difflib import unified_diff
from pathlib import Path

import boto3
import requests
from behave import given, then, when
from botocore.exceptions import ClientError
from helpers.dynamodb_data_generator import DateVariableResolver, JsonTestDataProcessor
from helpers.dynamodb_data_uploader import DynamoDBDataUploader

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# API endpoints
API_BASE_URL = os.getenv("API_BASE_URL", "https://" + "test" + ".eligibility-signposting-api.nhs.uk")

# SSM Parameter paths
SSM_BASE_PATH = "/" + "test" + "/mtls"
CERT_PARAMS = {
    "private_key": f"{SSM_BASE_PATH}/api_private_key_cert",
    "client_cert": f"{SSM_BASE_PATH}/api_client_cert",
    "ca_cert": f"{SSM_BASE_PATH}/api_ca_cert",
}


def remove_ignored_properties(obj):
    """
    Recursively remove specified properties from JSON objects before comparison.
    Creates deep copies to avoid modifying original data.

    Properties to remove:
    - meta (and all its sub-properties)
    - responseId

    Args:
        obj: The JSON object (dict, list, or primitive) to process

    Returns:
        A deep copy of the object with specified properties removed
    """
    if isinstance(obj, dict):
        # Create a new dict excluding the ignored properties
        filtered_dict = {}
        for key, value in obj.items():
            if key not in ["meta", "responseId"]:
                filtered_dict[key] = remove_ignored_properties(value)
        return filtered_dict
    if isinstance(obj, list):
        # Recursively process each item in the list
        return [remove_ignored_properties(item) for item in obj]
    # Return primitive values as-is
    return obj


@given("AWS credentials are loaded from the environment")
def step_impl_load_aws_credentials(context):
    """Load AWS credentials from environment variables."""
    context.aws_region = os.getenv("AWS_REGION", "eu-west-2")
    context.aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
    context.aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
    context.aws_session_token = os.getenv("AWS_SESSION_TOKEN")
    context.dynamodb_table_name = os.getenv("DYNAMODB_TABLE_NAME", "eligibilty_data_store")

    missing = []
    if not context.aws_region:
        missing.append("AWS_REGION")
    if not context.aws_access_key_id:
        missing.append("AWS_ACCESS_KEY_ID")
    if not context.aws_secret_access_key:
        missing.append("AWS_SECRET_ACCESS_KEY")

    assert not missing, f"Missing required environment variables: {', '.join(missing)}"

    logger.info("AWS credentials loaded successfully")


@given("mTLS certificates are downloaded and available in the out/ directory")
def step_impl_download_certificates(context):
    """Retrieve mTLS certs from SSM and write them to local files."""

    cert_dir = Path("./data/out")
    cert_dir.mkdir(parents=True, exist_ok=True)

    ssm = boto3.client(
        "ssm",
        region_name=context.aws_region,
        aws_access_key_id=context.aws_access_key_id,
        aws_secret_access_key=context.aws_secret_access_key,
        aws_session_token=context.aws_session_token,
    )

    context.cert_paths = {}
    for cert_type, param_name in CERT_PARAMS.items():
        cert_path = cert_dir / f"{cert_type}.pem"
        try:
            logger.info("Retrieving SSM parameter: %s", param_name)
            response = ssm.get_parameter(Name=param_name, WithDecryption=True)
            with cert_path.open("w") as f:
                f.write(response["Parameter"]["Value"])
            context.cert_paths[cert_type] = str(cert_path)
        except ClientError as e:
            msg = f"Failed to retrieve parameter {param_name}: {e}"
            raise RuntimeError(msg) from e

    logger.info("mTLS certificates written to local files")


@given("I generate the test data files")
def step_impl_generate_data(context):
    """Generate test data files with resolved <<DATE_...>> placeholders - once per feature."""
    if getattr(context, "feature_data_setup_done", False):
        logger.info("Test data already generated for this feature, skipping generation...")
        return

    logger.info("Generating test data files for feature...")
    input_dir = Path("data/in/dynamoDB").resolve()
    output_dir = Path("data/out/dynamoDB").resolve()

    resolver = DateVariableResolver()
    processor = JsonTestDataProcessor(input_dir, output_dir, resolver)

    if not input_dir.exists():
        logger.error("Input directory does not exist: %s", input_dir)
        return

    logger.info("Scanning for JSON files in directory: %s", input_dir)
    count = 0
    for root, _, files in os.walk(input_dir):
        for file in files:
            if file.endswith(".json"):
                full_path = Path(root) / file
                processor.process_file(full_path)
                count += 1

    if count == 0:
        logger.warning("No .json files found in %s", input_dir)
    else:
        logger.info("Processed %d test data file(s) for feature.", count)


@given("I upload the test data files to DynamoDB")
def step_impl_upload_data(context):
    """Upload generated test data to DynamoDB - once per feature."""
    if getattr(context, "feature_data_setup_done", False):
        logger.info("Test data already uploaded for this feature, skipping upload...")
        return

    logger.info("Uploading test data to DynamoDB for feature...")
    uploader = DynamoDBDataUploader(
        aws_region=context.aws_region,
        access_key=context.aws_access_key_id,
        secret_key=context.aws_secret_access_key,
        session_token=context.aws_session_token,
    )
    inserted = uploader.upload_files_from_path(table_name=context.dynamodb_table_name, path=Path("data/out/dynamoDB"))
    assert inserted > 0, "No data uploaded to DynamoDB"

    # Store for feature-level cleanup
    context.feature_uploader = uploader
    context.feature_dynamodb_items_count = inserted
    context.feature_data_setup_done = True

    logger.info("Uploaded %d items to DynamoDB (feature-level)", inserted)


@given('I have the NHS number "{nhs_number}"')
def step_impl_nhs_number(context, nhs_number):
    context.nhs_number = nhs_number


@then("I clean up DynamoDB test data")
def step_impl_cleanup_dynamo(context):
    """Clean up DynamoDB test data - handled at feature level now."""
    logger.info("DynamoDB cleanup will be handled at feature level - skipping scenario-level cleanup")
    # This step becomes a no-op since cleanup is handled in after_feature hook


def parse_headers_table(table):
    """Parse behave table into headers dictionary."""
    headers = {}
    if not table:
        return headers

    for row in table:
        if len(row.cells) != 2:
            raise ValueError(f"Expected 2 columns in headers table, got {len(row.cells)}")
        key, value = row.cells[0], row.cells[1]
        headers[key] = value
    return headers


def build_request_headers(context, dynamic_headers=None):
    """Build complete headers for API request."""
    # Start with required default header
    headers = {"nhs-login-nhs-number": context.nhs_number}

    # Add dynamic headers if provided
    if dynamic_headers:
        headers.update(dynamic_headers)

    return headers


def make_eligibility_api_call(context, headers):
    """Make mTLS call to Eligibility API with provided headers."""
    if not hasattr(context, "nhs_number"):
        msg = "NHS number not set in context."
        raise AssertionError(msg)

    if not hasattr(context, "cert_paths"):
        msg = "mTLS certificate paths not present in context."
        raise AssertionError(msg)

    api_url = f"{API_BASE_URL}/patient-check/{context.nhs_number}"
    cert = (context.cert_paths["client_cert"], context.cert_paths["private_key"])
    verify = False

    # Log headers for debugging (excluding sensitive values)
    safe_headers = {k: ("***" if "token" in k.lower() or "key" in k.lower() else v) for k, v in headers.items()}
    logger.info("Querying Eligibility API at %s with headers: %s", api_url, safe_headers)

    try:
        response = requests.get(api_url, cert=cert, verify=verify, timeout=30, headers=headers)
        context.response = response
        logger.info(
            "Querying Eligibility API response %s - %d",
            response.apparent_encoding,
            response.status_code,
        )
    except requests.exceptions.RequestException as e:
        msg = f"API request failed: {e}"
        raise RuntimeError(msg) from e


@when("I query the eligibility API")
def step_impl_call_eligibility_api(context):
    """Make mTLS call to Eligibility API using local certs and context NHS number."""
    headers = build_request_headers(context)
    make_eligibility_api_call(context, headers)


@when("I query the eligibility API using the headers:")
def step_impl_call_eligibility_api_with_headers(context):
    """Make mTLS call to Eligibility API with dynamic headers from table."""
    try:
        # Parse headers table
        dynamic_headers = parse_headers_table(context.table)
        logger.info("Parsed %d dynamic headers from table", len(dynamic_headers))

        # Build complete headers
        headers = build_request_headers(context, dynamic_headers)

        # Make API call
        make_eligibility_api_call(context, headers)

    except ValueError as e:
        msg = f"Invalid headers table format: {e}"
        raise AssertionError(msg) from e
    except Exception as e:
        msg = f"Failed to process headers table: {e}"
        raise RuntimeError(msg) from e


@then("the response status code should be {status_code:d}")
def step_impl_check_status_code(context, status_code):
    """Assert response HTTP status code."""
    if not hasattr(context, "response"):
        msg = "No HTTP response in context."
        raise AssertionError(msg)
    actual = context.response.status_code
    assert actual == status_code, f"Expected status {status_code}, got {actual}"


@then("the response should be valid JSON")
def step_impl_validate_json(context):
    """Assert that response content is valid JSON."""
    if not hasattr(context, "response"):
        msg = "No HTTP response in context."
        raise AssertionError(msg)

    try:
        context.json_response = context.response.json()
    except ValueError as e:
        msg = f"Response is not valid JSON: {e}"
        raise AssertionError(msg) from e


@then('the response should be matching the JSON "{json_response}"')
def step_impl_match_json_response(context, json_response):
    """
    Assert that the API response matches the expected JSON file.
    """
    expected_path = os.path.join(Path(__file__).parent.parent.parent, "data", "responses", json_response)
    if not os.path.isfile(expected_path):
        raise AssertionError(f"Expected JSON file not found: {expected_path}")

    with open(expected_path, encoding="utf-8") as f:
        expected = json.load(f)

    try:
        actual = context.response.json()
        print("> Actual JSON structure: ", actual)

        name, ext = os.path.splitext(json_response)
        actual_filename = f"{name}{ext}"
        actual_path = os.path.join(
            Path(__file__).parent.parent.parent,
            "data",
            "responses",
            actual_filename,
        )

        os.makedirs(os.path.dirname(actual_path), exist_ok=True)

        with open(actual_path, "w", encoding="utf-8") as f:
            json.dump(actual, f, indent=2, sort_keys=True)

    except Exception as e:
        raise AssertionError(f"Failed to parse API response as JSON: {e}")

    filtered_expected = remove_ignored_properties(expected)
    filtered_actual = remove_ignored_properties(actual)

    if filtered_expected != filtered_actual:
        expected_str = json.dumps(filtered_expected, indent=2, sort_keys=True)
        actual_str = json.dumps(filtered_actual, indent=2, sort_keys=True)
        diff = "\n".join(
            unified_diff(
                expected_str.splitlines(),
                actual_str.splitlines(),
                fromfile="expected",
                tofile="actual",
                lineterm="",
            )
        )
        raise AssertionError(f"Response JSON does not match expected.\nDiff:\n{diff}")
